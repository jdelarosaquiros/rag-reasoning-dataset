{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVGxKpsEQVwQ",
        "outputId": "4d3d12be-507f-4418-c4e1-c079a7ad852d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: datasets in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (2.2.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "af882e58c6ba40b09c39e60245a10c0a",
            "1a1c4f7bb5bd4de9a9ad48840c083f4d",
            "a4f75b88a1c94dc5b283a5fe3e3e358c",
            "1d7e734412e74dbba6b83ec230f7a256",
            "2635afd76a764cfbb6b71c0620ec9aaa",
            "922bdaaa91f64ccab134134ca815e643",
            "de561b8531774dc8b1591b5ddf7ea42b",
            "0471e4365767420ebe27397320526e93",
            "0e5e760208534671a54a324f01b7d49f",
            "ca1255319fb2459c843ea9bcadcd7e7a",
            "1226b0e4383b4af480b60171813916d7",
            "121b4d70936c43e7904822dfafb89db7",
            "16fe469ad2934ee1b679cbb04a936565",
            "8e0a5960a0e44a94ab9afb15bfe3e5a1",
            "0970f5b871ef4d51b9ebc5e0cb041dc3",
            "4d1a7d83760d4dd09058c0fcf07f51b8",
            "7491178b4af44e36bc634a2d99178a0b",
            "eee07fff3fa7403ca3a1e632ea66fcdc",
            "4c6c3ef997ca4063a231e13ce78f5df7",
            "7c5246a13c4f4c0791c4c90824a604d9",
            "ac647bd30c4b43c9a12e34c19e45949d",
            "a16ea07f75f8412482318893d0f38b1b",
            "e4e98913cc1f433497bb9e0499e1f400",
            "272d897accc040a1bed0153a8f888ccf",
            "8de6cee68aec46d1b09814265e6d2272",
            "193dd75743a04bf88306760e8649be4a",
            "8150fac807394b6daec7a75f623c7eaa",
            "987ea00117e34b65b29a186695d82f32",
            "b3e9fe3a3cce480e905700648189a608",
            "3c3c7b993136417881ae02e034d740ca",
            "7d4a2c6825c84a92a3be555c7cdbf213",
            "6bd14fddf2474460b6ee82126fad8f9e",
            "4427ffce45424dd0b3bc01441db86e9a"
          ]
        },
        "id": "6IAq-kZg9Mcl",
        "outputId": "56322a86-c8f0-4154-cc80-11b63b3b4bcd"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"selfrag/selfrag_train_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MNEA9bo4_iTY"
      },
      "outputs": [],
      "source": [
        "dataset = dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdVxO8mkRV_R",
        "outputId": "77a5f1da-da2c-4bcf-ef45-90f995af1020"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'output', 'input', 'id', 'dataset_name'],\n",
              "    num_rows: 145619\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeGKoWl2ReA0",
        "outputId": "863c2cf5-b330-48c7-e137-5d20b88ea01f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 100444\n",
            "Sum: 231873\n"
          ]
        }
      ],
      "source": [
        "token = \"[No Retrieval]\"\n",
        "no_retrive = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(no_retrive))\n",
        "print(\"Sum:\", sum(no_retrive))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCYtJmr1TCZh",
        "outputId": "7ab10dda-2d05-49ad-8dac-4b3d251b17a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 74219\n",
            "Sum: 204893\n"
          ]
        }
      ],
      "source": [
        "token = \"[Retrieval]\"\n",
        "retrive = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(retrive))\n",
        "print(\"Sum:\", sum(retrive))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th6Fsco3TE6Z",
        "outputId": "5e9e1678-aeb4-4c21-ead1-dadcde99cf36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 122599\n",
            "Sum: 122599\n"
          ]
        }
      ],
      "source": [
        "token = \"[Utility:5]\"\n",
        "ult_5 = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(ult_5))\n",
        "print(\"Sum:\", sum(ult_5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSmvtudxTiUi",
        "outputId": "88efc749-734e-46b5-c748-7fe4dbfde407"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 13050\n",
            "Sum: 13050\n"
          ]
        }
      ],
      "source": [
        "token = \"[Utility:4]\"\n",
        "ult_4 = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(ult_4))\n",
        "print(\"Sum:\", sum(ult_4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7jCAI-mTksy",
        "outputId": "8dd6dff4-0cbb-4de8-86db-919cde469d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 109\n",
            "Sum: 109\n"
          ]
        }
      ],
      "source": [
        "token = \"[Utility:3]\"\n",
        "ult_3 = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(ult_3))\n",
        "print(\"Sum:\", sum(ult_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgdnSV9ITlQC",
        "outputId": "a0e6fec0-d988-4eed-b3cd-ba8de3c989d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 6575\n",
            "Sum: 6575\n"
          ]
        }
      ],
      "source": [
        "token = \"[Utility:2]\"\n",
        "ult_2 = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(ult_2))\n",
        "print(\"Sum:\", sum(ult_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_O_T77BbTlqE",
        "outputId": "a96cc068-6f62-41e1-d0e9-c69441f5ce10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Records: 3286\n",
            "Sum: 3286\n"
          ]
        }
      ],
      "source": [
        "token = \"[Utility:1]\"\n",
        "ult_1 = [output.count(token) for output in dataset['output'] if output.count(token) > 0]\n",
        "print(\"Records:\", len(ult_1))\n",
        "print(\"Sum:\", sum(ult_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXg3CcNYd99f"
      },
      "source": [
        "## Generating Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "control_tokens = [\n",
        "    \"[Fully supported]\",\n",
        "    \"[Partially supported]\",\n",
        "    \"[No support / Contradictory]\",\n",
        "    \"[No Retrieval]\",\n",
        "    \"[Retrieval]\",\n",
        "    \"[Irrelevant]\",\n",
        "    \"[Relevant]\",\n",
        "    \"<paragraph>\",\n",
        "    \"</paragraph>\",\n",
        "    \"[Utility:1]\",\n",
        "    \"[Utility:2]\",\n",
        "    \"[Utility:3]\",\n",
        "    \"[Utility:4]\",\n",
        "    \"[Utility:5]\",\n",
        "    \"[Continue to Use Evidence]\",\n",
        "    \"[Completeness: Complete]\",\n",
        "    \"[Completeness: Partially Complete]\",\n",
        "    \"[Completeness: Incomplete]\",\n",
        "]\n",
        "\n",
        "relevant_tokens = [\"Relevant\", \"Irrelevant\"]\n",
        "support_tokens = [\n",
        "    \"Fully supported\",\n",
        "    \"Partially supported\",\n",
        "    \"No support / Contradictory\",\n",
        "]\n",
        "utility_tokens = [\n",
        "    \"Utility:5\",\n",
        "    \"Utility:4\",\n",
        "    \"Utility:3\",\n",
        "    \"Utility:2\",\n",
        "    \"Utility:1\",\n",
        "]\n",
        "retrieval_tokens = [\"No Retrieval\", \"Retrieval\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def postprocess_answer_option_conditioned(answer, filter_paragraph=True):\n",
        "\n",
        "    if filter_paragraph:\n",
        "        answer = re.sub(r\"<paragraph>.*?</paragraph>\", \"\", answer, flags=re.DOTALL)\n",
        "    for token in control_tokens:\n",
        "        answer = answer.replace(token, \"\")\n",
        "\n",
        "    if \"</s>\" in answer:\n",
        "        answer = answer.replace(\"</s>\", \"\")\n",
        "\n",
        "    if \"<|endoftext|>\" in answer:\n",
        "        answer = answer.replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "G63GbL6BWaRb"
      },
      "outputs": [],
      "source": [
        "complete_samples = []\n",
        "partially_complete_samples = []\n",
        "incomplete_samples = []\n",
        "retrieve_samples = []\n",
        "no_retrieve_samples = []\n",
        "relevant_samples = []\n",
        "irrelevant_samples = []\n",
        "supported_samples = []\n",
        "partially_supported_samples = []\n",
        "not_supported_samples = []\n",
        "processed_samples = []\n",
        "\n",
        "for question, input, output in zip(dataset['instruction'], dataset['input'], dataset['output']):\n",
        "  output = output.replace(\"[Utility:5]\", \"[Completeness: Complete]\")\n",
        "  output = output.replace(\"[Utility:4]\", \"[Completeness: Partially Complete]\")\n",
        "  output = output.replace(\"[Utility:3]\", \"[Completeness: Incomplete]\")\n",
        "  output = output.replace(\"[Utility:2]\", \"[Completeness: Incomplete]\")\n",
        "  output = output.replace(\"[Utility:1]\", \"[Completeness: Incomplete]\")\n",
        "\n",
        "  row_data = {\"instruction\": question, \"input\": input, \"output\": output}\n",
        "\n",
        "  # Completeness Task\n",
        "  if(\"[Completeness: Complete]\" in output):\n",
        "    complete_samples.append(output)\n",
        "    row_data[\"completeness\"] = [{\"answer\": postprocess_answer_option_conditioned(output),\"label\": \"Complete\"}]\n",
        "  if(\"[Completeness: Partially Complete]\" in output):\n",
        "    partially_complete_samples.append(output)\n",
        "    row_data[\"completeness\"] = [{\"answer\": postprocess_answer_option_conditioned(output),\"label\": \"Partially Complete\"}]\n",
        "  if(\"[Completeness: Incomplete]\" in output):\n",
        "    incomplete_samples.append(output)\n",
        "    row_data[\"completeness\"] = [{\"answer\": postprocess_answer_option_conditioned(output),\"label\": \"Incomplete\"}]\n",
        "\n",
        "  # Relevancy Task\n",
        "  data = re.findall(f\"(<paragraph>.*?\\[({'|'.join(relevant_tokens)})\\])\", output, flags=re.DOTALL) # Get list of the following tuple: (paragraph, relevancy)\n",
        "  data = {postprocess_answer_option_conditioned(sample[0], False):sample[1] for sample in data}\n",
        "\n",
        "  row_data[\"relevancy\"] = [{\"paragraph\": paragraph, \"label\": data[paragraph]} for paragraph in data]\n",
        "\n",
        "  # Is Supported Task\n",
        "\n",
        "  support_task_data = []\n",
        "  data = re.findall(f\"(<paragraph>.*?\\[({'|'.join(support_tokens)})\\])\", output, flags=re.DOTALL) # Get list of the following tuple: (paragraph + answer, is supported)\n",
        "\n",
        "  for sample in data:\n",
        "    inputs = sample[0].split(\"</paragraph>\")\n",
        "    label = sample[1]\n",
        "\n",
        "    paragraph = postprocess_answer_option_conditioned(inputs[0], False)\n",
        "    answer = postprocess_answer_option_conditioned(inputs[1])\n",
        "\n",
        "    support_task_data.append({\"paragraph\": paragraph, \"answer\": answer, \"label\": label})\n",
        "  \n",
        "  row_data[\"support\"] = support_task_data\n",
        "  \n",
        "  processed_samples.append(row_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Reasoning for Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: huggingface_hub in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (0.20.3)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from huggingface_hub) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests->huggingface_hub) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed huggingface_hub-0.22.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipywidgets) (8.22.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipywidgets) (5.14.1)\n",
            "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: decorator in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.42)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: exceptiongroup in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: executing>=1.2.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /root/miniconda3/envs/self_rag/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = \"[YOUR_HF_TOKEN]\"\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-29 22:26:10 llm_engine.py:73] Initializing an LLM engine with config: model='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir='/gscratch/h2lab/akari/model_cache', load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 04-29 22:26:15 llm_engine.py:223] # GPU blocks: 8025, # CPU blocks: 2048\n",
            "INFO 04-29 22:26:18 model_runner.py:394] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
            "INFO 04-29 22:26:27 model_runner.py:437] Graph capturing finished in 9 secs.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from vllm import LLM, SamplingParams\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\", token=hf_token)\n",
        "model = LLM(\n",
        "    model_id,\n",
        "    download_dir=\"/gscratch/h2lab/akari/model_cache\",\n",
        "    gpu_memory_utilization=0.90,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def completeness_prompt(instruction, answer, label):\n",
        "    return f'''You will be provided an Instruction, an Answer, and an evaluation on the Completeness of the answer. You will explain the reason why the Answer was given the provided Completeness evaluation.\n",
        "Instruction:\n",
        "{instruction}\n",
        "Answer:\n",
        "{answer}\n",
        "Completeness:\n",
        "{label}\n",
        "Reason:\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You a helpful and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "You will be provided an Instruciton, an Answer, and an evaluation on the Completeness of the answer. You will explain the reason why the Answer was given the provided Completeness evaluation.\n",
            "Instructiopn:\n",
            "Create a regex expression to remove all characters in a string after the last dot in the string. For example, if I have a pattern of \"X.Y.4\" I want the number 4 and the dot to be removed.\n",
            "Answer:\n",
            "One possible regex expression to remove all characters in a string after the last dot in the string is:\n",
            "\n",
            "`\\.[^.]*$`\n",
            "\n",
            "This expression matches a dot (`\\.`) followed by zero or more characters that are not dots (`[^.]*`) until the end of the string (`$`).For example, if you have a pattern of “X.Y.4”, this expression will match “.4” and replace it with “”, resulting in “X.Y”.\n",
            "\n",
            "Here is some code in Python that demonstrates how to use this expression:\n",
            "```python\n",
            "import re\n",
            "\n",
            "pattern = \"X.Y.4\"\n",
            "regex = r\"\\.[^.]*$\"\n",
            "result = re.sub(regex, \"\", pattern)\n",
            "print(result) # X.Y\n",
            "```\n",
            "Completeness:\n",
            "Complete\n",
            "Reason:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": completeness_prompt(sample['instruction'], sample['completeness'][0]['answer'], sample['completeness'][0]['label'])},\n",
        "]\n",
        "\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "terminators = [\n",
        "    tokenizer.eos_token,\n",
        "    \"<|eot_id|>\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6, \n",
        "    top_p=0.9, \n",
        "    max_tokens=258, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: The Completeness evaluation is \"Complete\" because the answer provides a thorough explanation of the regex expression and how it works. The answer explains the purpose of each part of the expression, including the dot (`\\.`), the zero or more characters that are not dots (`[^.]*`), and the end of the string (`$`). This provides a clear understanding of how the expression will match and remove characters in the input string.\n",
            "\n",
            "Additionally, the answer provides a code snippet in Python that demonstrates how to use the regex expression with the `re.sub` function to remove the characters after the last dot in the string. This code snippet is concise and easy to understand, making it a complete answer to the problem.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_prompts_batch = []\n",
        "for sample in processed_samples:\n",
        "    for data in sample['completeness']:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": completeness_prompt(sample['instruction'], data['answer'], data['label'])},\n",
        "        ]\n",
        "\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        completeness_prompts_batch.append(prompt)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou a helpful and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou will be provided an Instruciton, an Answer, and an evaluation on the Completeness of the answer. You will explain the reason why the Answer was given the provided Completeness evaluation.\\nInstructiopn:\\nIn this task, you are given a context paragraph of the tweet and question. Your task is to generate right answer of given question based on given context tweet paragraph.\\n\\nExample input: Context: Our prayers are with the students, educators & families at Independence High School & all the first responders on the scene. #PatriotPride— Doug Ducey (@dougducey) February 12, 2016 Question: at which school were first responders on the scene for?\\nExample output: independence high school\\nExample explanation: From the context tweet, we can see that independence high school is the right answer.\\nQ: Context: BREAKING: ATF sending additional agents from Portland along with K9 team to #UCCShooting tragedy.— ATF HQ (@ATFHQ) October 1, 2015 Question: where are the agents from?\\nA:\\nAnswer:\\nportland\\nCompleteness:\\nComplete\\nReason:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completeness_prompts_batch[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 145619/145619 [2:21:42<00:00, 17.13it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: I'm ready to assist!\n",
            "\n",
            "The answer is: portland\n",
            "\n",
            "The completeness evaluation is: Complete\n",
            "\n",
            "Reason: The context tweet clearly states that the agents are being sent from Portland, along with a K9 team, to the UCCShooting tragedy. This information directly answers the question \"where are the agents from?\", making the answer complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "preds = model.generate(prompts=completeness_prompts_batch, sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "preds = [pred.outputs[0].text for pred in preds]\n",
        "\n",
        "with open('completeness_reasoning.json', 'w') as f:\n",
        "    json.dump(preds, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "for sample in processed_samples:\n",
        "    for data in sample['completeness']:\n",
        "        data['reason'] = preds[i]\n",
        "        i += 1\n",
        "\n",
        "with open('data_checkpoint.json', 'w') as f:\n",
        "    json.dump(processed_samples, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "145619\n"
          ]
        }
      ],
      "source": [
        "# Load Checkpoint\n",
        "import json\n",
        "\n",
        "with open('data_checkpoint.json') as f:\n",
        "    processed_samples = json.load(f)\n",
        "\n",
        "print(len(processed_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['instruction', 'output', 'input', 'id', 'dataset_name'],\n",
              "    num_rows: 145619\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Removing Alpaca Data\n",
        "reduced_samples = []\n",
        "\n",
        "for sample, dataset_name in zip(processed_samples, dataset['dataset_name']):\n",
        "    if(dataset_name != \"gpt4_alpaca\"):\n",
        "        reduced_samples.append(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reducing Data\n",
        "import random\n",
        "\n",
        "reduced_samples_length = 50000\n",
        "\n",
        "reduced_sample_indexes = random.sample(range(len(reduced_samples)), reduced_samples_length)\n",
        "reduced_samples = [reduced_samples[i] for i in reduced_sample_indexes]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relevancy_prompt(instruction, paragraph, label):\n",
        "    return f'''You will be provided an Instruction, a Paragraph, and an evaluation on the Relevancy of the paragraph with respect to the question. You will explain the reason why the Paragraph was given the provided Relevancy evaluation.\n",
        "Instruction:\n",
        "{instruction}\n",
        "Paragraph:\n",
        "{paragraph}\n",
        "Relevancy:\n",
        "{label}\n",
        "Reason:\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "relevancy_prompts_batch = []\n",
        "\n",
        "for sample in reduced_samples:\n",
        "    for data in sample['relevancy']:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": relevancy_prompt(sample['instruction'], data['paragraph'], data['label'])},\n",
        "        ]\n",
        "\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "\n",
        "        relevancy_prompts_batch.append(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou a helpful and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou will be provided an Instruciton, a Paragraph, and an evaluation on the Relevancy of the paragraph with respect to the question. You will explain the reason why the Paragraph was given the provided Relevancy evaluation.\\nInstructiopn:\\nWhy do many people believe that ivermectin can prevent or treat Covid-19?\\nParagraph:\\nIvermectin\\nin humans in the treatment of onchocerciasis (river blindness), but is also effective against other worm infestations (such as strongyloidiasis, ascariasis, trichuriasis, filariasis and enterobiasis), and some epidermal parasitic skin diseases, including scabies. Ivermectin is currently being used to help eliminate river blindness (onchocerciasis) in the Americas, and to stop transmission of lymphatic filariasis and onchocerciasis around the world in programs sponsored by the Carter Center using ivermectin donated by Merck. The disease is common in 30 African countries, six Latin American countries, and Yemen. The drug rapidly kills microfilariae, but not the adult worms. A single oral dose of\\nRelevancy:\\nRelevant\\nReason:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevancy_prompts_batch[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 47738/47738 [48:11<00:00, 16.51it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: The paragraph is considered \"Relevant\" to the instruction \"Why do many people believe that ivermectin can prevent or treat Covid-19?\" because it provides information about the uses of ivermectin, a drug that is being researched and discussed for its potential to prevent or treat Covid-19. Although the paragraph does not directly mention Covid-19, it highlights the drug's effectiveness against various parasitic infections and diseases, which is the foundation of the claims being made about its potential use against Covid-19. The information in the paragraph provides context and background knowledge about ivermectin's properties and uses, which are relevant to understanding why people may believe it can be used to prevent or treat Covid-19.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "preds = model.generate(prompts=relevancy_prompts_batch, sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "preds = [pred.outputs[0].text for pred in preds]\n",
        "\n",
        "with open('relevancy_reasoning.json', 'w') as f:\n",
        "    json.dump(preds, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47738\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for sample in reduced_samples:\n",
        "    for data in sample['relevancy']:\n",
        "        data['reason'] = preds[i]\n",
        "        i += 1\n",
        "\n",
        "with open('data_checkpoint.json', 'w') as f:\n",
        "    json.dump(reduced_samples, f)\n",
        "\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def support_prompt(paragraph, answer, label):\n",
        "    return f'''You will be provided an Paragraph, an Answer, and an evaluation on whether the Answer is supported by the Paragraph. You will explain the reason why the Answer was given the provided Is Supported evaluation.\n",
        "Paragraph:\n",
        "{paragraph}\n",
        "Answer:\n",
        "{answer}\n",
        "Is Supported:\n",
        "{label}\n",
        "Reason:\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "support_prompts_batch = []\n",
        "\n",
        "for sample in reduced_samples:\n",
        "    for data in sample['support']:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": support_prompt(data['paragraph'], data['answer'], data['label'])},\n",
        "        ]\n",
        "\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False, \n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "        support_prompts_batch.append(prompt)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou a helpful and honest assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou will be provided an Paragraph, an Answer, and an evaluation on whether the Answer is supported by the Paragraph. You will explain the reason why the Answer was given the provided Is Supported evaluation.\\nParagraph:\\nIvermectin\\nin humans in the treatment of onchocerciasis (river blindness), but is also effective against other worm infestations (such as strongyloidiasis, ascariasis, trichuriasis, filariasis and enterobiasis), and some epidermal parasitic skin diseases, including scabies. Ivermectin is currently being used to help eliminate river blindness (onchocerciasis) in the Americas, and to stop transmission of lymphatic filariasis and onchocerciasis around the world in programs sponsored by the Carter Center using ivermectin donated by Merck. The disease is common in 30 African countries, six Latin American countries, and Yemen. The drug rapidly kills microfilariae, but not the adult worms. A single oral dose of\\nAnswer:\\nThe belief that ivermectin can prevent or treat Covid-19 is based on a combination of anecdotal reports, observational studies, and laboratory experiments, but the evidence supporting its use is limited and inconclusive.\\n\\n\\nIs Supported:\\nNo support / Contradictory\\nReason:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "support_prompts_batch[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 46471/46471 [42:23<00:00, 18.27it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: The Answer is evaluated as \"No support / Contradictory\".\n",
            "\n",
            "The reason is that the Paragraph does not mention anything about ivermectin being used to prevent or treat Covid-19. The paragraph only discusses the use of ivermectin in the treatment of various worm infestations and parasitic skin diseases, as well as its use in eliminating river blindness and stopping the transmission of lymphatic filariasis and onchocerciasis. There is no mention of ivermectin being used to prevent or treat Covid-19, which is the topic of the Answer. Therefore, the Answer is not supported by the provided paragraph.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "preds = model.generate(prompts=support_prompts_batch, sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "preds = [pred.outputs[0].text for pred in preds]\n",
        "\n",
        "with open('support_reasoning.json', 'w') as f:\n",
        "    json.dump(preds, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46471\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "for sample in reduced_samples:\n",
        "    for data in sample['support']:\n",
        "        data['reason'] = preds[i]\n",
        "        i += 1\n",
        "\n",
        "with open('data_checkpoint.json', 'w') as f:\n",
        "    json.dump(reduced_samples, f)\n",
        "\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "completeness_instruction = \"You will be provided an Instruciton, an Answer, and an evaluation on the Completeness of the answer. You will explain the reason why the Answer was given the provided Completeness evaluation.\"\n",
        "\n",
        "relevancy_instruction = \"You will be provided an Instruciton, a Paragraph, and an evaluation on the Relevancy of the paragraph with respect to the question. You will explain the reason why the Paragraph was given the provided Relevancy evaluation.\"\n",
        "\n",
        "support_instruction = \"You will be provided an Paragraph, an Answer, and an evaluation on whether the Answer is supported by the Paragraph. You will explain the reason why the Answer was given the provided Is Supported evaluation.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Genrate Completeness Instrucitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Write 20 different ways of asking the following instruction:\\n\\n{completeness_instruction}\"},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6, \n",
        "    top_p=0.9, \n",
        "    max_tokens=4028, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: Here are 20 different ways of asking the instruction:\n",
            "\n",
            "1. Please provide a task, response, and assessment of the response's thoroughness. Explain why the response was given the assigned completeness score.\n",
            "2. You will be given a task, answer, and evaluation of the answer's completeness. Please explain the reasoning behind the completeness assessment.\n",
            "3. For each task, you will receive an answer, a completeness evaluation, and a justification for the evaluation.\n",
            "4. Please provide a detailed explanation for each answer, including the completeness score and the reasoning behind it.\n",
            "5. You will be asked to review tasks with answers, completeness evaluations, and explanations for each evaluation.\n",
            "6. Provide a breakdown of each answer, including the completeness score and a justification for the evaluation.\n",
            "7. For each response, please explain the completeness score and the reasoning behind it.\n",
            "8. You will receive answers, completeness evaluations, and explanations for each evaluation. Please review and provide feedback.\n",
            "9. Please review the answers, completeness evaluations, and explanations for each evaluation, and provide your thoughts on the reasoning behind each score.\n",
            "10. You will be given tasks, answers, and completeness evaluations. Please provide a detailed explanation for each answer, including the completeness score and the reasoning.\n",
            "11. Provide a thorough explanation for each answer, including the completeness score and a justification for the evaluation.\n",
            "12. Please review the answers, completeness evaluations, and explanations, and provide your thoughts on the completeness of each response.\n",
            "13. You will receive answers, completeness evaluations, and explanations. Please provide a detailed breakdown of each answer, including the completeness score and the reasoning.\n",
            "14. For each task, you will receive an answer, a completeness evaluation, and a justification for the evaluation. Please review and provide feedback.\n",
            "15. Please provide a detailed explanation for each answer, including the completeness score and the reasoning behind it.\n",
            "16. You will be given a task, answer, and completeness evaluation. Please explain the reasoning behind the completeness assessment and provide feedback.\n",
            "17. Provide a thorough breakdown of each answer, including the completeness score and a justification for the evaluation.\n",
            "18. You will receive answers, completeness evaluations, and explanations. Please review and provide your thoughts on the completeness of each response.\n",
            "19. Please review the answers, completeness evaluations, and explanations, and provide a detailed explanation for each answer, including the completeness score and the reasoning.\n",
            "20. You will be asked to review tasks with answers, completeness evaluations, and explanations. Please provide a detailed breakdown of each answer, including the completeness score and the justification for the evaluation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"Please provide a task, response, and assessment of the response's thoroughness. Explain why the response was given the assigned completeness score.\",\n",
              " \"You will be given a task, answer, and evaluation of the answer's completeness. Please explain the reasoning behind the completeness assessment.\",\n",
              " 'For each task, you will receive an answer, a completeness evaluation, and a justification for the evaluation.',\n",
              " 'Please provide a detailed explanation for each answer, including the completeness score and the reasoning behind it.',\n",
              " 'You will be asked to review tasks with answers, completeness evaluations, and explanations for each evaluation.',\n",
              " 'Provide a breakdown of each answer, including the completeness score and a justification for the evaluation.',\n",
              " 'For each response, please explain the completeness score and the reasoning behind it.',\n",
              " 'You will receive answers, completeness evaluations, and explanations for each evaluation. Please review and provide feedback.',\n",
              " 'Please review the answers, completeness evaluations, and explanations for each evaluation, and provide your thoughts on the reasoning behind each score.',\n",
              " 'You will be given tasks, answers, and completeness evaluations. Please provide a detailed explanation for each answer, including the completeness score and the reasoning.',\n",
              " 'Provide a thorough explanation for each answer, including the completeness score and a justification for the evaluation.',\n",
              " 'Please review the answers, completeness evaluations, and explanations, and provide your thoughts on the completeness of each response.',\n",
              " 'You will receive answers, completeness evaluations, and explanations. Please provide a detailed breakdown of each answer, including the completeness score and the reasoning.',\n",
              " 'For each task, you will receive an answer, a completeness evaluation, and a justification for the evaluation. Please review and provide feedback.',\n",
              " 'Please provide a detailed explanation for each answer, including the completeness score and the reasoning behind it.',\n",
              " 'You will be given a task, answer, and completeness evaluation. Please explain the reasoning behind the completeness assessment and provide feedback.',\n",
              " 'Provide a thorough breakdown of each answer, including the completeness score and a justification for the evaluation.',\n",
              " 'You will receive answers, completeness evaluations, and explanations. Please review and provide your thoughts on the completeness of each response.',\n",
              " 'Please review the answers, completeness evaluations, and explanations, and provide a detailed explanation for each answer, including the completeness score and the reasoning.',\n",
              " 'You will be asked to review tasks with answers, completeness evaluations, and explanations. Please provide a detailed breakdown of each answer, including the completeness score and the justification for the evaluation.']"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "questions_v1 = [re.sub(r\"\\d+\\. \", \"\", line) for line in preds[0].outputs[0].text.splitlines() if re.match(r\"^\\d+\", line)]\n",
        "questions_v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: Here are 20 different ways to ask the instruction:\n",
            "\n",
            "1. Can you provide a breakdown of how you evaluated the completeness of the answer and the reasoning behind it?\n",
            "2. How do you determine the completeness of an answer, and can you explain why you gave it the score you did?\n",
            "3. Can you walk me through your thought process when evaluating the completeness of an answer and provide the reasoning behind your decision?\n",
            "4. What criteria do you use to assess the completeness of an answer, and how do you apply them to this particular response?\n",
            "5. Can you explain why you gave the answer a completeness score of [X] and provide evidence to support your evaluation?\n",
            "6. How do you ensure that your evaluation of completeness is accurate and unbiased, and can you provide an example of this process in action?\n",
            "7. Can you provide a step-by-step guide on how you evaluate the completeness of an answer, including the reasoning behind each step?\n",
            "8. What are the key factors you consider when evaluating the completeness of an answer, and how do you weigh their importance?\n",
            "9. Can you provide an example of an answer that you would give a high completeness score to, and explain why you would make that evaluation?\n",
            "10. How do you handle situations where an answer may not be 100% complete but still provides valuable information?\n",
            "11. Can you explain how your evaluation of completeness takes into account the specific context of the question and the answer?\n",
            "12. What are some common pitfalls to avoid when evaluating the completeness of an answer, and how do you ensure you avoid them?\n",
            "13. Can you provide a template or framework that you use to evaluate the completeness of an answer, and explain how it works?\n",
            "14. How do you distinguish between an answer that is simply incomplete and one that is inadequate or poorly constructed?\n",
            "15. Can you explain why you might give an answer a completeness score of 0, and what it means for the overall quality of the response?\n",
            "16. How do you use your knowledge of the topic and the question being asked to inform your evaluation of the answer's completeness?\n",
            "17. Can you provide an example of an answer that you would give a low completeness score to, and explain why you would make that evaluation?\n",
            "18. What are some strategies you use to encourage answers that are more complete and comprehensive?\n",
            "19. Can you explain how your evaluation of completeness is influenced by the answer's relevance to the question, and how you weigh this factor?\n",
            "20. How do you ensure that your evaluation of completeness is fair and consistent across all answers, and what steps do you take to prevent bias?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=1, \n",
        "    top_p=0.9, \n",
        "    max_tokens=4028, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")\n",
        "\n",
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Can you provide a breakdown of how you evaluated the completeness of the answer and the reasoning behind it?',\n",
              " 'How do you determine the completeness of an answer, and can you explain why you gave it the score you did?',\n",
              " 'Can you walk me through your thought process when evaluating the completeness of an answer and provide the reasoning behind your decision?',\n",
              " 'What criteria do you use to assess the completeness of an answer, and how do you apply them to this particular response?',\n",
              " 'Can you explain why you gave the answer a completeness score of [X] and provide evidence to support your evaluation?',\n",
              " 'How do you ensure that your evaluation of completeness is accurate and unbiased, and can you provide an example of this process in action?',\n",
              " 'Can you provide a step-by-step guide on how you evaluate the completeness of an answer, including the reasoning behind each step?',\n",
              " 'What are the key factors you consider when evaluating the completeness of an answer, and how do you weigh their importance?',\n",
              " 'Can you provide an example of an answer that you would give a high completeness score to, and explain why you would make that evaluation?',\n",
              " 'How do you handle situations where an answer may not be 100% complete but still provides valuable information?',\n",
              " 'Can you explain how your evaluation of completeness takes into account the specific context of the question and the answer?',\n",
              " 'What are some common pitfalls to avoid when evaluating the completeness of an answer, and how do you ensure you avoid them?',\n",
              " 'Can you provide a template or framework that you use to evaluate the completeness of an answer, and explain how it works?',\n",
              " 'How do you distinguish between an answer that is simply incomplete and one that is inadequate or poorly constructed?',\n",
              " 'Can you explain why you might give an answer a completeness score of 0, and what it means for the overall quality of the response?',\n",
              " \"How do you use your knowledge of the topic and the question being asked to inform your evaluation of the answer's completeness?\",\n",
              " 'Can you provide an example of an answer that you would give a low completeness score to, and explain why you would make that evaluation?',\n",
              " 'What are some strategies you use to encourage answers that are more complete and comprehensive?',\n",
              " \"Can you explain how your evaluation of completeness is influenced by the answer's relevance to the question, and how you weigh this factor?\",\n",
              " 'How do you ensure that your evaluation of completeness is fair and consistent across all answers, and what steps do you take to prevent bias?']"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "questions_v2 = [re.sub(r\"\\d+\\. \", \"\", line) for line in preds[0].outputs[0].text.splitlines() if re.match(r\"^\\d+\", line)]\n",
        "questions_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[17, 3, 11, 13, 10, 9, 15, 18, 8, 7]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['What are some strategies you use to encourage answers that are more complete and comprehensive?',\n",
              " 'What criteria do you use to assess the completeness of an answer, and how do you apply them to this particular response?',\n",
              " 'What are some common pitfalls to avoid when evaluating the completeness of an answer, and how do you ensure you avoid them?',\n",
              " 'How do you distinguish between an answer that is simply incomplete and one that is inadequate or poorly constructed?',\n",
              " 'Can you explain how your evaluation of completeness takes into account the specific context of the question and the answer?',\n",
              " 'How do you handle situations where an answer may not be 100% complete but still provides valuable information?',\n",
              " \"How do you use your knowledge of the topic and the question being asked to inform your evaluation of the answer's completeness?\",\n",
              " \"Can you explain how your evaluation of completeness is influenced by the answer's relevance to the question, and how you weigh this factor?\",\n",
              " 'Can you provide an example of an answer that you would give a high completeness score to, and explain why you would make that evaluation?',\n",
              " 'What are the key factors you consider when evaluating the completeness of an answer, and how do you weigh their importance?',\n",
              " 'You will receive answers, completeness evaluations, and explanations. Please review and provide your thoughts on the completeness of each response.',\n",
              " 'Please provide a detailed explanation for each answer, including the completeness score and the reasoning behind it.',\n",
              " 'Please review the answers, completeness evaluations, and explanations, and provide your thoughts on the completeness of each response.',\n",
              " 'For each task, you will receive an answer, a completeness evaluation, and a justification for the evaluation. Please review and provide feedback.',\n",
              " 'Provide a thorough explanation for each answer, including the completeness score and a justification for the evaluation.',\n",
              " 'You will be given tasks, answers, and completeness evaluations. Please provide a detailed explanation for each answer, including the completeness score and the reasoning.',\n",
              " 'You will be given a task, answer, and completeness evaluation. Please explain the reasoning behind the completeness assessment and provide feedback.',\n",
              " 'Please review the answers, completeness evaluations, and explanations, and provide a detailed explanation for each answer, including the completeness score and the reasoning.',\n",
              " 'Please review the answers, completeness evaluations, and explanations for each evaluation, and provide your thoughts on the reasoning behind each score.',\n",
              " 'You will receive answers, completeness evaluations, and explanations for each evaluation. Please review and provide feedback.']"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_quesiton_indexes = random.sample(range(20), 10)\n",
        "print(random_quesiton_indexes)\n",
        "\n",
        "completeness_instructions = [questions_v2[i] for i in random_quesiton_indexes] + [questions_v1[i] for i in random_quesiton_indexes]\n",
        "completeness_instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Relevancy Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Write 20 different ways of asking the following instruction:\\n\\n{relevancy_instruction}\"},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: Here are 20 different ways of asking the instruction:\n",
            "\n",
            "1. Please review the provided paragraph and evaluate its relevance to the given instruction. Explain your reasoning.\n",
            "2. Assess the paragraph's relevance to the question and provide a justification for your answer.\n",
            "3. Determine the degree of relevance between the paragraph and the instruction, and explain your thought process.\n",
            "4. Evaluate the paragraph's connection to the instruction and provide a brief explanation.\n",
            "5. Provide a relevance score for the paragraph and justify your decision.\n",
            "6. Review the paragraph and explain how well it addresses the instruction.\n",
            "7. Assess the paragraph's relevance to the instruction and provide a written justification.\n",
            "8. Determine the paragraph's relevance to the question and provide a clear explanation.\n",
            "9. Evaluate the paragraph's connection to the instruction and provide a concise explanation.\n",
            "10. Provide a rating for the paragraph's relevance to the instruction and justify your choice.\n",
            "11. Review the paragraph and explain how it relates to the instruction.\n",
            "12. Assess the paragraph's relevance to the instruction and provide a logical explanation.\n",
            "13. Determine the paragraph's relevance to the question and provide a detailed explanation.\n",
            "14. Evaluate the paragraph's connection to the instruction and provide a clear and concise explanation.\n",
            "15. Provide a relevance score for the paragraph and justify your decision with evidence.\n",
            "16. Review the paragraph and explain its relevance to the instruction.\n",
            "17. Assess the paragraph's relevance to the instruction and provide a thorough explanation.\n",
            "18. Determine the paragraph's relevance to the question and provide a well-supported explanation.\n",
            "19. Evaluate the paragraph's connection to the instruction and provide a logical and clear explanation.\n",
            "20. Provide a rating for the paragraph's relevance to the instruction and justify your choice with specific examples.\n",
            "\n",
            "These different phrasings aim to convey the same instruction, but with slight variations in wording and emphasis.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6, \n",
        "    top_p=0.9, \n",
        "    max_tokens=4028, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")\n",
        "\n",
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please review the provided paragraph and evaluate its relevance to the given instruction. Explain your reasoning.',\n",
              " \"Assess the paragraph's relevance to the question and provide a justification for your answer.\",\n",
              " 'Determine the degree of relevance between the paragraph and the instruction, and explain your thought process.',\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a brief explanation.\",\n",
              " 'Provide a relevance score for the paragraph and justify your decision.',\n",
              " 'Review the paragraph and explain how well it addresses the instruction.',\n",
              " \"Assess the paragraph's relevance to the instruction and provide a written justification.\",\n",
              " \"Determine the paragraph's relevance to the question and provide a clear explanation.\",\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a concise explanation.\",\n",
              " \"Provide a rating for the paragraph's relevance to the instruction and justify your choice.\",\n",
              " 'Review the paragraph and explain how it relates to the instruction.',\n",
              " \"Assess the paragraph's relevance to the instruction and provide a logical explanation.\",\n",
              " \"Determine the paragraph's relevance to the question and provide a detailed explanation.\",\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a clear and concise explanation.\",\n",
              " 'Provide a relevance score for the paragraph and justify your decision with evidence.',\n",
              " 'Review the paragraph and explain its relevance to the instruction.',\n",
              " \"Assess the paragraph's relevance to the instruction and provide a thorough explanation.\",\n",
              " \"Determine the paragraph's relevance to the question and provide a well-supported explanation.\",\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a logical and clear explanation.\",\n",
              " \"Provide a rating for the paragraph's relevance to the instruction and justify your choice with specific examples.\"]"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "questions_v1 = [re.sub(r\"\\d+\\. \", \"\", line) for line in preds[0].outputs[0].text.splitlines() if re.match(r\"^\\d+\", line)]\n",
        "questions_v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: Here are 20 different ways of asking the instruction:\n",
            "\n",
            "1. Please review the given instruction, paragraph, and relevancy evaluation, and provide a clear explanation for why the paragraph received its assigned score.\n",
            "2. Explain the reasoning behind the relevancy evaluation given to the provided paragraph.\n",
            "3. Write a justification for the relevancy rating assigned to the paragraph.\n",
            "4. Provide a detailed explanation for why the paragraph is or is not relevant to the given instruction.\n",
            "5. Elucidate the thought process behind the relevancy evaluation of the paragraph.\n",
            "6. Discuss the connection between the paragraph and the instruction, highlighting the reasons for the given relevancy score.\n",
            "7. Interpret the relevancy evaluation assigned to the paragraph and provide supporting evidence from the text.\n",
            "8. Explain how the paragraph relates to the instruction and justify the assigned relevancy score.\n",
            "9. Provide a breakdown of the reasons behind the relevancy rating, using the paragraph and instruction as evidence.\n",
            "10. Write a summary of the relevancy evaluation and explain the thought process behind the decision.\n",
            "11. Explain the relevance of the paragraph to the instruction, using the evaluation as a guide.\n",
            "12. Discuss the paragraph's connection to the instruction, highlighting areas of relevance and irrelevance.\n",
            "13. Provide a step-by-step explanation of the relevancy evaluation process and justify the assigned score.\n",
            "14. Elucidate the relevance of the paragraph to the instruction, highlighting the most significant points of connection.\n",
            "15. Explain the relationship between the paragraph and instruction, highlighting areas of agreement and disagreement.\n",
            "16. Write a detailed analysis of the relevancy evaluation, including the reasons for the assigned score.\n",
            "17. Provide an explanation for the relevancy rating, using specific examples from the paragraph and instruction.\n",
            "18. Discuss the paragraph's relevance to the instruction, highlighting the most important points of connection.\n",
            "19. Provide a clear and concise explanation of the thought process behind the relevancy evaluation.\n",
            "20. Elucidate the reasons why the paragraph received the assigned relevancy score, using the instruction and paragraph as evidence.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=1, \n",
        "    top_p=0.9, \n",
        "    max_tokens=4028, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")\n",
        "\n",
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please review the given instruction, paragraph, and relevancy evaluation, and provide a clear explanation for why the paragraph received its assigned score.',\n",
              " 'Explain the reasoning behind the relevancy evaluation given to the provided paragraph.',\n",
              " 'Write a justification for the relevancy rating assigned to the paragraph.',\n",
              " 'Provide a detailed explanation for why the paragraph is or is not relevant to the given instruction.',\n",
              " 'Elucidate the thought process behind the relevancy evaluation of the paragraph.',\n",
              " 'Discuss the connection between the paragraph and the instruction, highlighting the reasons for the given relevancy score.',\n",
              " 'Interpret the relevancy evaluation assigned to the paragraph and provide supporting evidence from the text.',\n",
              " 'Explain how the paragraph relates to the instruction and justify the assigned relevancy score.',\n",
              " 'Provide a breakdown of the reasons behind the relevancy rating, using the paragraph and instruction as evidence.',\n",
              " 'Write a summary of the relevancy evaluation and explain the thought process behind the decision.',\n",
              " 'Explain the relevance of the paragraph to the instruction, using the evaluation as a guide.',\n",
              " \"Discuss the paragraph's connection to the instruction, highlighting areas of relevance and irrelevance.\",\n",
              " 'Provide a step-by-step explanation of the relevancy evaluation process and justify the assigned score.',\n",
              " 'Elucidate the relevance of the paragraph to the instruction, highlighting the most significant points of connection.',\n",
              " 'Explain the relationship between the paragraph and instruction, highlighting areas of agreement and disagreement.',\n",
              " 'Write a detailed analysis of the relevancy evaluation, including the reasons for the assigned score.',\n",
              " 'Provide an explanation for the relevancy rating, using specific examples from the paragraph and instruction.',\n",
              " \"Discuss the paragraph's relevance to the instruction, highlighting the most important points of connection.\",\n",
              " 'Provide a clear and concise explanation of the thought process behind the relevancy evaluation.',\n",
              " 'Elucidate the reasons why the paragraph received the assigned relevancy score, using the instruction and paragraph as evidence.']"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "questions_v2 = [re.sub(r\"\\d+\\. \", \"\", line) for line in preds[0].outputs[0].text.splitlines() if re.match(r\"^\\d+\", line)]\n",
        "questions_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14, 16, 8, 1, 0, 18, 11, 6, 13, 10]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Explain the relationship between the paragraph and instruction, highlighting areas of agreement and disagreement.',\n",
              " 'Provide an explanation for the relevancy rating, using specific examples from the paragraph and instruction.',\n",
              " 'Provide a breakdown of the reasons behind the relevancy rating, using the paragraph and instruction as evidence.',\n",
              " 'Explain the reasoning behind the relevancy evaluation given to the provided paragraph.',\n",
              " 'Please review the given instruction, paragraph, and relevancy evaluation, and provide a clear explanation for why the paragraph received its assigned score.',\n",
              " 'Provide a clear and concise explanation of the thought process behind the relevancy evaluation.',\n",
              " \"Discuss the paragraph's connection to the instruction, highlighting areas of relevance and irrelevance.\",\n",
              " 'Interpret the relevancy evaluation assigned to the paragraph and provide supporting evidence from the text.',\n",
              " 'Elucidate the relevance of the paragraph to the instruction, highlighting the most significant points of connection.',\n",
              " 'Explain the relevance of the paragraph to the instruction, using the evaluation as a guide.',\n",
              " 'Provide a relevance score for the paragraph and justify your decision with evidence.',\n",
              " \"Assess the paragraph's relevance to the instruction and provide a thorough explanation.\",\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a concise explanation.\",\n",
              " \"Assess the paragraph's relevance to the question and provide a justification for your answer.\",\n",
              " 'Please review the provided paragraph and evaluate its relevance to the given instruction. Explain your reasoning.',\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a logical and clear explanation.\",\n",
              " \"Assess the paragraph's relevance to the instruction and provide a logical explanation.\",\n",
              " \"Assess the paragraph's relevance to the instruction and provide a written justification.\",\n",
              " \"Evaluate the paragraph's connection to the instruction and provide a clear and concise explanation.\",\n",
              " 'Review the paragraph and explain how it relates to the instruction.']"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_quesiton_indexes = random.sample(range(20), 10)\n",
        "print(random_quesiton_indexes)\n",
        "\n",
        "relevancy_instructions = [questions_v2[i] for i in random_quesiton_indexes] + [questions_v1[i] for i in random_quesiton_indexes]\n",
        "relevancy_instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Support Instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You a helpful and honest assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Write 20 different ways of asking the following instruction:\\n\\n{support_instruction}\"},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False, \n",
        "    add_generation_prompt=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: Here are 20 different ways of asking the instruction:\n",
            "\n",
            "1. Analyze the given paragraph and answer, determining if the answer is supported by the provided text.\n",
            "2. Evaluate the answer provided and explain whether it is supported by the given paragraph.\n",
            "3. Determine the validity of the answer based on the paragraph and explain the reasoning.\n",
            "4. Assess the answer and provide evidence from the paragraph to support or refute the evaluation.\n",
            "5. Examine the paragraph and answer, and indicate whether the answer is supported by the text.\n",
            "6. Provide a justification for the evaluation of the answer based on the paragraph.\n",
            "7. Evaluate the answer and explain whether it is supported by the paragraph, citing specific evidence.\n",
            "8. Determine if the answer is supported by the paragraph, and provide a brief explanation.\n",
            "9. Analyze the paragraph and answer, and provide a conclusion on whether the answer is supported.\n",
            "10. Provide a written explanation of why the answer was given the \"Is Supported\" evaluation.\n",
            "11. Determine whether the answer is supported by the paragraph, and provide evidence to support your claim.\n",
            "12. Evaluate the answer and provide a brief summary of the paragraph, highlighting the relevant points.\n",
            "13. Assess the answer and provide a detailed explanation of why it is (or is not) supported by the paragraph.\n",
            "14. Examine the paragraph and answer, and indicate whether the answer is supported by the text, providing evidence.\n",
            "15. Provide a justification for the evaluation of the answer, citing specific sentences or phrases from the paragraph.\n",
            "16. Evaluate the answer and explain whether it is supported by the paragraph, using quotes or references to the text.\n",
            "17. Determine if the answer is supported by the paragraph, and provide a concise summary of the relevant points.\n",
            "18. Analyze the paragraph and answer, and provide a conclusion on whether the answer is supported, citing specific evidence.\n",
            "19. Provide a written explanation of why the answer was given the \"Is Supported\" evaluation, using the paragraph as evidence.\n",
            "20. Evaluate the answer and provide a detailed analysis of the paragraph, highlighting the relevant points that support or refute the answer.\n",
            "\n",
            "Let me know if you need anything else!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=0.6, \n",
        "    top_p=0.9, \n",
        "    max_tokens=4028, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")\n",
        "\n",
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Analyze the given paragraph and answer, determining if the answer is supported by the provided text.',\n",
              " 'Evaluate the answer provided and explain whether it is supported by the given paragraph.',\n",
              " 'Determine the validity of the answer based on the paragraph and explain the reasoning.',\n",
              " 'Assess the answer and provide evidence from the paragraph to support or refute the evaluation.',\n",
              " 'Examine the paragraph and answer, and indicate whether the answer is supported by the text.',\n",
              " 'Provide a justification for the evaluation of the answer based on the paragraph.',\n",
              " 'Evaluate the answer and explain whether it is supported by the paragraph, citing specific evidence.',\n",
              " 'Determine if the answer is supported by the paragraph, and provide a brief explanation.',\n",
              " 'Analyze the paragraph and answer, and provide a conclusion on whether the answer is supported.',\n",
              " 'Provide a written explanation of why the answer was given the \"Is Supported\" evaluation.',\n",
              " 'Determine whether the answer is supported by the paragraph, and provide evidence to support your claim.',\n",
              " 'Evaluate the answer and provide a brief summary of the paragraph, highlighting the relevant points.',\n",
              " 'Assess the answer and provide a detailed explanation of why it is (or is not) supported by the paragraph.',\n",
              " 'Examine the paragraph and answer, and indicate whether the answer is supported by the text, providing evidence.',\n",
              " 'Provide a justification for the evaluation of the answer, citing specific sentences or phrases from the paragraph.',\n",
              " 'Evaluate the answer and explain whether it is supported by the paragraph, using quotes or references to the text.',\n",
              " 'Determine if the answer is supported by the paragraph, and provide a concise summary of the relevant points.',\n",
              " 'Analyze the paragraph and answer, and provide a conclusion on whether the answer is supported, citing specific evidence.',\n",
              " 'Provide a written explanation of why the answer was given the \"Is Supported\" evaluation, using the paragraph as evidence.',\n",
              " 'Evaluate the answer and provide a detailed analysis of the paragraph, highlighting the relevant points that support or refute the answer.']"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "questions_v1 = [re.sub(r\"\\d+\\. \", \"\", line) for line in preds[0].outputs[0].text.splitlines() if re.match(r\"^\\d+\", line)]\n",
        "questions_v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model prediction: Here are 20 different ways to ask the instruction:\n",
            "\n",
            "1. Please review the given paragraph and answer, and then indicate whether the answer is supported by the paragraph and explain your reasoning.\n",
            "2. Evaluate the relationship between the paragraph and the answer, and determine if the answer is supported by the text.\n",
            "3. Assess the answer's alignment with the provided paragraph, and provide a justification for your evaluation.\n",
            "4. Analyze the answer and paragraph to determine if they are logically connected, and explain the outcome.\n",
            "5. Check if the answer is backed up by the paragraph, and provide a clear explanation for your assessment.\n",
            "6. Investigate the correspondence between the paragraph and answer, and determine if the answer is supported.\n",
            "7. Evaluate the paragraph and answer to determine if the answer is supported by the paragraph, and provide a justification for your evaluation.\n",
            "8. Determine if the answer is supported by the paragraph, and explain the reasoning behind your conclusion.\n",
            "9. Review the paragraph and answer to assess whether the answer is logically derived from the text.\n",
            "10. Evaluate the coherence between the paragraph and answer, and determine if the answer is supported.\n",
            "11. Analyze the answer and paragraph to determine if they are causally linked, and explain the outcome.\n",
            "12. Determine if the answer is backed up by the paragraph, and provide a clear explanation for your assessment.\n",
            "13. Evaluate the degree to which the paragraph supports the answer, and explain your reasoning.\n",
            "14. Review the paragraph and answer to determine if the answer is supported by the paragraph's content.\n",
            "15. Assess the connection between the paragraph and answer, and determine if the answer is supported by the text.\n",
            "16. Evaluate the answer's relation to the paragraph, and determine if the answer is supported.\n",
            "17. Investigate the degree to which the paragraph supports the answer, and provide a clear explanation for your assessment.\n",
            "18. Determine if the answer is logically deduced from the paragraph, and explain your conclusion.\n",
            "19. Review the paragraph and answer to assess whether the answer is consistent with the text.\n",
            "20. Evaluate the paragraph and answer to determine if the answer is a direct consequence of the paragraph's content, and explain your reasoning.\n",
            "\n",
            "These different ways of asking the instruction provide various nuances and focus areas for the evaluation task, such as logical connection, coherence, causal link, and textual support.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature=1, \n",
        "    top_p=0.9, \n",
        "    max_tokens=4028, \n",
        "    skip_special_tokens=False,\n",
        "    stop=terminators\n",
        ")\n",
        "\n",
        "preds = model.generate(prompts=[prompt], sampling_params=sampling_params)\n",
        "print(f\"Model prediction: {preds[0].outputs[0].text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please review the given paragraph and answer, and then indicate whether the answer is supported by the paragraph and explain your reasoning.',\n",
              " 'Evaluate the relationship between the paragraph and the answer, and determine if the answer is supported by the text.',\n",
              " \"Assess the answer's alignment with the provided paragraph, and provide a justification for your evaluation.\",\n",
              " 'Analyze the answer and paragraph to determine if they are logically connected, and explain the outcome.',\n",
              " 'Check if the answer is backed up by the paragraph, and provide a clear explanation for your assessment.',\n",
              " 'Investigate the correspondence between the paragraph and answer, and determine if the answer is supported.',\n",
              " 'Evaluate the paragraph and answer to determine if the answer is supported by the paragraph, and provide a justification for your evaluation.',\n",
              " 'Determine if the answer is supported by the paragraph, and explain the reasoning behind your conclusion.',\n",
              " 'Review the paragraph and answer to assess whether the answer is logically derived from the text.',\n",
              " 'Evaluate the coherence between the paragraph and answer, and determine if the answer is supported.',\n",
              " 'Analyze the answer and paragraph to determine if they are causally linked, and explain the outcome.',\n",
              " 'Determine if the answer is backed up by the paragraph, and provide a clear explanation for your assessment.',\n",
              " 'Evaluate the degree to which the paragraph supports the answer, and explain your reasoning.',\n",
              " \"Review the paragraph and answer to determine if the answer is supported by the paragraph's content.\",\n",
              " 'Assess the connection between the paragraph and answer, and determine if the answer is supported by the text.',\n",
              " \"Evaluate the answer's relation to the paragraph, and determine if the answer is supported.\",\n",
              " 'Investigate the degree to which the paragraph supports the answer, and provide a clear explanation for your assessment.',\n",
              " 'Determine if the answer is logically deduced from the paragraph, and explain your conclusion.',\n",
              " 'Review the paragraph and answer to assess whether the answer is consistent with the text.',\n",
              " \"Evaluate the paragraph and answer to determine if the answer is a direct consequence of the paragraph's content, and explain your reasoning.\"]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "questions_v2 = [re.sub(r\"\\d+\\. \", \"\", line) for line in preds[0].outputs[0].text.splitlines() if re.match(r\"^\\d+\", line)]\n",
        "questions_v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 15, 10, 7, 17, 11, 13, 1, 3, 9]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Please review the given paragraph and answer, and then indicate whether the answer is supported by the paragraph and explain your reasoning.',\n",
              " \"Evaluate the answer's relation to the paragraph, and determine if the answer is supported.\",\n",
              " 'Analyze the answer and paragraph to determine if they are causally linked, and explain the outcome.',\n",
              " 'Determine if the answer is supported by the paragraph, and explain the reasoning behind your conclusion.',\n",
              " 'Determine if the answer is logically deduced from the paragraph, and explain your conclusion.',\n",
              " 'Determine if the answer is backed up by the paragraph, and provide a clear explanation for your assessment.',\n",
              " \"Review the paragraph and answer to determine if the answer is supported by the paragraph's content.\",\n",
              " 'Evaluate the relationship between the paragraph and the answer, and determine if the answer is supported by the text.',\n",
              " 'Analyze the answer and paragraph to determine if they are logically connected, and explain the outcome.',\n",
              " 'Evaluate the coherence between the paragraph and answer, and determine if the answer is supported.',\n",
              " 'Analyze the given paragraph and answer, determining if the answer is supported by the provided text.',\n",
              " 'Evaluate the answer and explain whether it is supported by the paragraph, using quotes or references to the text.',\n",
              " 'Determine whether the answer is supported by the paragraph, and provide evidence to support your claim.',\n",
              " 'Determine if the answer is supported by the paragraph, and provide a brief explanation.',\n",
              " 'Analyze the paragraph and answer, and provide a conclusion on whether the answer is supported, citing specific evidence.',\n",
              " 'Evaluate the answer and provide a brief summary of the paragraph, highlighting the relevant points.',\n",
              " 'Examine the paragraph and answer, and indicate whether the answer is supported by the text, providing evidence.',\n",
              " 'Evaluate the answer provided and explain whether it is supported by the given paragraph.',\n",
              " 'Assess the answer and provide evidence from the paragraph to support or refute the evaluation.',\n",
              " 'Provide a written explanation of why the answer was given the \"Is Supported\" evaluation.']"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "random_quesiton_indexes = random.sample(range(20), 10)\n",
        "print(random_quesiton_indexes)\n",
        "\n",
        "support_instructions = [questions_v2[i] for i in random_quesiton_indexes] + [questions_v1[i] for i in random_quesiton_indexes]\n",
        "support_instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Adding Root Instruction\n",
        "\n",
        "completeness_instructions.append(completeness_instruction)\n",
        "relevancy_instructions.append(relevancy_instruction)\n",
        "support_instructions.append(support_instruction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribute Quesitons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_instrucitons = len(completeness_instructions) # All Instruciton Types Have Same Length\n",
        "for sample in reduced_samples:\n",
        "    for data in sample['completeness']:\n",
        "        rand_index = random.sample(range(num_instrucitons), 1)[0]\n",
        "        data['instruction'] = completeness_instructions[rand_index]\n",
        "\n",
        "    for data in sample['relevancy']:\n",
        "        rand_index = random.sample(range(num_instrucitons), 1)[0]\n",
        "        data['instruction'] = relevancy_instructions[rand_index]\n",
        "    \n",
        "    for data in sample['support']:\n",
        "        rand_index = random.sample(range(num_instrucitons), 1)[0]\n",
        "        data['instruction'] = support_instructions[rand_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'paragraph': 'Ivermectin\\nin humans in the treatment of onchocerciasis (river blindness), but is also effective against other worm infestations (such as strongyloidiasis, ascariasis, trichuriasis, filariasis and enterobiasis), and some epidermal parasitic skin diseases, including scabies. Ivermectin is currently being used to help eliminate river blindness (onchocerciasis) in the Americas, and to stop transmission of lymphatic filariasis and onchocerciasis around the world in programs sponsored by the Carter Center using ivermectin donated by Merck. The disease is common in 30 African countries, six Latin American countries, and Yemen. The drug rapidly kills microfilariae, but not the adult worms. A single oral dose of',\n",
              " 'label': 'Relevant',\n",
              " 'reason': 'The paragraph is considered \"Relevant\" to the instruction \"Why do many people believe that ivermectin can prevent or treat Covid-19?\" because it provides information about the uses of ivermectin, a drug that is being researched and discussed for its potential to prevent or treat Covid-19. Although the paragraph does not directly mention Covid-19, it highlights the drug\\'s effectiveness against various parasitic infections and diseases, which is the foundation of the claims being made about its potential use against Covid-19. The information in the paragraph provides context and background knowledge about ivermectin\\'s properties and uses, which are relevant to understanding why people may believe it can be used to prevent or treat Covid-19.',\n",
              " 'instruction': 'Explain the reasoning behind the relevancy evaluation given to the provided paragraph.'}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reduced_samples[0]['relevancy'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "def completeness_text_prompt(prompt_instruciton, instruction, answer, label, reason):\n",
        "    return f'''{prompt_instruciton}\n",
        "Instruction:\n",
        "{instruction}\n",
        "Answer:\n",
        "{answer}\n",
        "Reason:\n",
        "{reason}\n",
        "Completeness:\n",
        "{label}'''\n",
        "\n",
        "def relevancy_text_prompt(prompt_instruciton, instruction, paragraph, label, reason):\n",
        "    return f'''{prompt_instruciton}\n",
        "Instruction:\n",
        "{instruction}\n",
        "Paragraph:\n",
        "{paragraph}\n",
        "Reason:\n",
        "{reason}\n",
        "Relevancy:\n",
        "{label}'''\n",
        "\n",
        "def support_text_prompt(prompt_instruciton, paragraph, answer, label, reason):\n",
        "    return f'''{prompt_instruciton}\n",
        "Paragraph:\n",
        "{paragraph}\n",
        "Answer:\n",
        "{answer}\n",
        "Reason:\n",
        "{reason}\n",
        "Is Supported:\n",
        "{label}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "def completeness_input_prompt(instruction, answer):\n",
        "    return f'''Instruction:\n",
        "{instruction}\n",
        "Answer:\n",
        "{answer}'''\n",
        "\n",
        "def relevancy_input_prompt(instruction, paragraph):\n",
        "    return f'''Instruction:\n",
        "{instruction}\n",
        "Paragraph:\n",
        "{paragraph}'''\n",
        "\n",
        "def support_input_prompt(paragraph, answer):\n",
        "    return f'''Paragraph:\n",
        "{paragraph}\n",
        "Answer:\n",
        "{answer}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def completeness_output_prompt(label, reason):\n",
        "    return f'''Reason:\n",
        "{reason}\n",
        "Completeness:\n",
        "{label}'''\n",
        "\n",
        "def relevancy_output_prompt(label, reason):\n",
        "    return f'''Reason:\n",
        "{reason}\n",
        "Relevancy:\n",
        "{label}'''\n",
        "\n",
        "def support_output_prompt(label, reason):\n",
        "    return f'''Reason:\n",
        "{reason}\n",
        "Is Supported:\n",
        "{label}'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'instruction': 'You will be provided an Instruciton, an Answer, and an evaluation on the Completeness of the answer. You will explain the reason why the Answer was given the provided Completeness evaluation.', 'inputs': \"Instruction:\\nWhy do many people believe that ivermectin can prevent or treat Covid-19?\\nAnswer:\\nThe belief that ivermectin can prevent or treat Covid-19 is based on a combination of anecdotal reports, observational studies, and laboratory experiments, but the evidence supporting its use is limited and inconclusive.\\n\\nIvermectin is an anti-parasitic drug that has been used for decades to treat conditions such as river blindness, scabies, and lice infestations.In recent years, there have been some studies suggesting that it may also have antiviral properties, but most of these studies have been conducted in vitro or in animal models, and the evidence is not strong enough to support its use as a treatment for Covid-19 in humans.\\n\\nHowever, most health authorities, including the World Health Organization and the US Food and Drug Administration, have not approved ivermectin for the treatment of Covid-19, citing the lack of strong evidence and the potential risks of using the drug without proper medical supervision.\\n\\nIt's important to note that the best way to prevent and treat Covid-19 is through proven measures such as vaccination, social distancing, wearing masks, and following other public health guidelines.If you have any concerns about Covid-19 or its treatment, you should consult a healthcare professional for advice.\", 'output': 'Reason:\\nComplete\\nCompleteness:\\nThe Completeness evaluation is \"Complete\" because the answer provides a thorough and comprehensive explanation of why many people believe that ivermectin can prevent or treat Covid-19. The answer:\\n\\n1. Acknowledges the sources of the belief, including anecdotal reports, observational studies, and laboratory experiments.\\n2. Provides context about ivermectin\\'s history and usage, highlighting its effectiveness against parasitic infections.\\n3. Discusses the limitations of the evidence supporting its use against Covid-19, including the lack of strong evidence and the need for further research.\\n4. Cites the stance of reputable health authorities, such as the World Health Organization and the US Food and Drug Administration, on the lack of approval for ivermectin as a treatment for Covid-19.\\n5. Offers a clear and concise summary of the best methods for preventing and treating Covid-19, including vaccination, social distancing, and following public health guidelines.\\n6. Concludes by emphasizing the importance of consulting a healthcare professional for advice on Covid-19 treatment and prevention.\\n\\nThe answer provides a well-rounded and informative explanation, covering the various aspects of the topic, making it a complete response.', 'text': 'You will be provided an Instruciton, an Answer, and an evaluation on the Completeness of the answer. You will explain the reason why the Answer was given the provided Completeness evaluation.\\nInstruction:\\nWhy do many people believe that ivermectin can prevent or treat Covid-19?\\nAnswer:\\nThe belief that ivermectin can prevent or treat Covid-19 is based on a combination of anecdotal reports, observational studies, and laboratory experiments, but the evidence supporting its use is limited and inconclusive.\\n\\nIvermectin is an anti-parasitic drug that has been used for decades to treat conditions such as river blindness, scabies, and lice infestations.In recent years, there have been some studies suggesting that it may also have antiviral properties, but most of these studies have been conducted in vitro or in animal models, and the evidence is not strong enough to support its use as a treatment for Covid-19 in humans.\\n\\nHowever, most health authorities, including the World Health Organization and the US Food and Drug Administration, have not approved ivermectin for the treatment of Covid-19, citing the lack of strong evidence and the potential risks of using the drug without proper medical supervision.\\n\\nIt\\'s important to note that the best way to prevent and treat Covid-19 is through proven measures such as vaccination, social distancing, wearing masks, and following other public health guidelines.If you have any concerns about Covid-19 or its treatment, you should consult a healthcare professional for advice.\\nReason:\\nThe Completeness evaluation is \"Complete\" because the answer provides a thorough and comprehensive explanation of why many people believe that ivermectin can prevent or treat Covid-19. The answer:\\n\\n1. Acknowledges the sources of the belief, including anecdotal reports, observational studies, and laboratory experiments.\\n2. Provides context about ivermectin\\'s history and usage, highlighting its effectiveness against parasitic infections.\\n3. Discusses the limitations of the evidence supporting its use against Covid-19, including the lack of strong evidence and the need for further research.\\n4. Cites the stance of reputable health authorities, such as the World Health Organization and the US Food and Drug Administration, on the lack of approval for ivermectin as a treatment for Covid-19.\\n5. Offers a clear and concise summary of the best methods for preventing and treating Covid-19, including vaccination, social distancing, and following public health guidelines.\\n6. Concludes by emphasizing the importance of consulting a healthcare professional for advice on Covid-19 treatment and prevention.\\n\\nThe answer provides a well-rounded and informative explanation, covering the various aspects of the topic, making it a complete response.\\nCompleteness:\\nComplete'}\n"
          ]
        }
      ],
      "source": [
        "new_dataset = []\n",
        "\n",
        "for sample in reduced_samples:\n",
        "    for data in sample['completeness']:\n",
        "        data['text'] = completeness_text_prompt(data['instruction'], sample['instruction'], data['answer'], data['label'], data['reason'])\n",
        "        new_dataset.append({\n",
        "            \"instruction\": data['instruction'],\n",
        "            \"inputs\": completeness_input_prompt(sample['instruction'], data['answer']),\n",
        "            \"output\": completeness_output_prompt(data['reason'], data['label']),\n",
        "            \"text\": data['text']\n",
        "        })\n",
        "\n",
        "    for data in sample['relevancy']:\n",
        "        data['text'] = relevancy_text_prompt(data['instruction'], sample['instruction'], data['paragraph'], data['label'], data['reason'])\n",
        "        new_dataset.append({\n",
        "            \"instruction\": data['instruction'],\n",
        "            \"inputs\": relevancy_input_prompt(sample['instruction'], data['paragraph']),\n",
        "            \"output\": relevancy_output_prompt(data['reason'], data['label']),\n",
        "            \"text\": data['text']\n",
        "        })\n",
        "    \n",
        "    for data in sample['support']:\n",
        "        data['text'] = support_text_prompt(data['instruction'], data['paragraph'], data['answer'], data['label'], data['reason'])\n",
        "        new_dataset.append({\n",
        "            \"instruction\": data['instruction'],\n",
        "            \"inputs\": support_input_prompt(data['paragraph'], data['answer']),\n",
        "            \"output\": support_output_prompt(data['reason'], data['label']),\n",
        "            \"text\": data['text']\n",
        "        })\n",
        "        \n",
        "print(new_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Dataset\n",
        "with open('rag_reasoning_dataset.json', 'w') as f:\n",
        "    json.dump(new_dataset, f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0471e4365767420ebe27397320526e93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0970f5b871ef4d51b9ebc5e0cb041dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac647bd30c4b43c9a12e34c19e45949d",
            "placeholder": "​",
            "style": "IPY_MODEL_a16ea07f75f8412482318893d0f38b1b",
            "value": " 256M/256M [00:04&lt;00:00, 73.2MB/s]"
          }
        },
        "0e5e760208534671a54a324f01b7d49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "121b4d70936c43e7904822dfafb89db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16fe469ad2934ee1b679cbb04a936565",
              "IPY_MODEL_8e0a5960a0e44a94ab9afb15bfe3e5a1",
              "IPY_MODEL_0970f5b871ef4d51b9ebc5e0cb041dc3"
            ],
            "layout": "IPY_MODEL_4d1a7d83760d4dd09058c0fcf07f51b8"
          }
        },
        "1226b0e4383b4af480b60171813916d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16fe469ad2934ee1b679cbb04a936565": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7491178b4af44e36bc634a2d99178a0b",
            "placeholder": "​",
            "style": "IPY_MODEL_eee07fff3fa7403ca3a1e632ea66fcdc",
            "value": "Downloading data: 100%"
          }
        },
        "193dd75743a04bf88306760e8649be4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bd14fddf2474460b6ee82126fad8f9e",
            "placeholder": "​",
            "style": "IPY_MODEL_4427ffce45424dd0b3bc01441db86e9a",
            "value": " 145619/145619 [00:02&lt;00:00, 101152.64 examples/s]"
          }
        },
        "1a1c4f7bb5bd4de9a9ad48840c083f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922bdaaa91f64ccab134134ca815e643",
            "placeholder": "​",
            "style": "IPY_MODEL_de561b8531774dc8b1591b5ddf7ea42b",
            "value": "Downloading readme: 100%"
          }
        },
        "1d7e734412e74dbba6b83ec230f7a256": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca1255319fb2459c843ea9bcadcd7e7a",
            "placeholder": "​",
            "style": "IPY_MODEL_1226b0e4383b4af480b60171813916d7",
            "value": " 1.26k/1.26k [00:00&lt;00:00, 28.3kB/s]"
          }
        },
        "2635afd76a764cfbb6b71c0620ec9aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "272d897accc040a1bed0153a8f888ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_987ea00117e34b65b29a186695d82f32",
            "placeholder": "​",
            "style": "IPY_MODEL_b3e9fe3a3cce480e905700648189a608",
            "value": "Generating train split: 100%"
          }
        },
        "3c3c7b993136417881ae02e034d740ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4427ffce45424dd0b3bc01441db86e9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c6c3ef997ca4063a231e13ce78f5df7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d1a7d83760d4dd09058c0fcf07f51b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd14fddf2474460b6ee82126fad8f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7491178b4af44e36bc634a2d99178a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c5246a13c4f4c0791c4c90824a604d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d4a2c6825c84a92a3be555c7cdbf213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8150fac807394b6daec7a75f623c7eaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de6cee68aec46d1b09814265e6d2272": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c3c7b993136417881ae02e034d740ca",
            "max": 145619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d4a2c6825c84a92a3be555c7cdbf213",
            "value": 145619
          }
        },
        "8e0a5960a0e44a94ab9afb15bfe3e5a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c6c3ef997ca4063a231e13ce78f5df7",
            "max": 255748659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c5246a13c4f4c0791c4c90824a604d9",
            "value": 255748659
          }
        },
        "922bdaaa91f64ccab134134ca815e643": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987ea00117e34b65b29a186695d82f32": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16ea07f75f8412482318893d0f38b1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4f75b88a1c94dc5b283a5fe3e3e358c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0471e4365767420ebe27397320526e93",
            "max": 1265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e5e760208534671a54a324f01b7d49f",
            "value": 1265
          }
        },
        "ac647bd30c4b43c9a12e34c19e45949d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af882e58c6ba40b09c39e60245a10c0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a1c4f7bb5bd4de9a9ad48840c083f4d",
              "IPY_MODEL_a4f75b88a1c94dc5b283a5fe3e3e358c",
              "IPY_MODEL_1d7e734412e74dbba6b83ec230f7a256"
            ],
            "layout": "IPY_MODEL_2635afd76a764cfbb6b71c0620ec9aaa"
          }
        },
        "b3e9fe3a3cce480e905700648189a608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca1255319fb2459c843ea9bcadcd7e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de561b8531774dc8b1591b5ddf7ea42b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4e98913cc1f433497bb9e0499e1f400": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_272d897accc040a1bed0153a8f888ccf",
              "IPY_MODEL_8de6cee68aec46d1b09814265e6d2272",
              "IPY_MODEL_193dd75743a04bf88306760e8649be4a"
            ],
            "layout": "IPY_MODEL_8150fac807394b6daec7a75f623c7eaa"
          }
        },
        "eee07fff3fa7403ca3a1e632ea66fcdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
